경사 하강법 (Gradient descent method)
	그래프의 최소점에 도달하게 되면 학습을 종료
	랜덤으로 한 점으로부터 시작합니다. 좌우로 조금씩 그리고 한번씩 움직이면서 이전 값보다 작아지는지를 관찰
	목표는 이 손실 함수의 최소점인 Global cost minimum을 찾는 것

	한 칸씩 움직이는 스텝(Learning rate)를 잘못 설정할 경우 Local cost minimum에 빠질 가능성이 높음
	
	Local cost minimum - 그래프에서 움푹 파여서 최저점으로 인식이 될수 있는 곳

	한칸씩 전진하는 단위를 Learning rate	
		적당한 Learning rate를 찾는 노가다가 필수적
		너무 작으면 - 시간이 오래걸림
		너무 크면 - 우리가 찾으려는 최소값을 지나치고 검은 점은 계속 진동하다가 발산하게(Overshooting) 될수 있음(Overshooting)
	
		